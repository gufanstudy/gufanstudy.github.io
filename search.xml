<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python爬虫(三)：网络请求模块</title>
      <link href="/2022/05/15/python%E7%88%AC%E8%99%AB(%E4%B8%89)%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9D%97/"/>
      <url>/2022/05/15/python%E7%88%AC%E8%99%AB(%E4%B8%89)%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h4 id="urllib模块简介"><a href="#urllib模块简介" class="headerlink" title="urllib模块简介"></a>urllib模块简介</h4><blockquote><ol><li><p>urllib是什么？</p><p>Python内置的一个网络请求模块</p></li><li><p>为什么要学习它？</p><ul><li>对比的去学习第三方的（学习方法）</li><li>有些爬虫项目 用的就是urllib</li><li>有的时候需要 urllib + requests 二者配合使用</li></ul></li></ol></blockquote><h4 id="urllib模块快速入门"><a href="#urllib模块快速入门" class="headerlink" title="urllib模块快速入门"></a>urllib模块快速入门</h4><h5 id="urllib-request-urlopen"><a href="#urllib-request-urlopen" class="headerlink" title="urllib.request.urlopen()"></a>urllib.request.urlopen()</h5><p><strong>获取响应对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># urllib.request.urlopen(&#x27;网站&#x27;)</span></span><br><span class="line"><span class="comment"># 作用：向网站发起请求获得响应</span></span><br><span class="line"><span class="comment"># response为响应对象</span></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取状态码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;状态码：%s&#x27;</span>%(response.getcode()))</span><br><span class="line"><span class="comment"># 获取请求的url地址</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;url地址：%s&quot;</span>%(response.geturl()))</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220512205428649.png" alt="image-20220512205428649"></p><h5 id="urllib-request-Request"><a href="#urllib-request-Request" class="headerlink" title="urllib.request.Request()"></a>urllib.request.Request()</h5><p><strong>创建请求对象</strong></p><p>爬取百度首页源码</p><ol><li>创建请求对象 构建User-Agent</li><li>获取请求对象 urlopen()</li><li>read().decode(‘utf-8’) 读取响应对象的内容</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建请求对象</span></span><br><span class="line">req = urllib.request.Request(url,headers=headers)</span><br><span class="line"><span class="comment"># 2. 获取响应对象</span></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line"><span class="comment"># 3. read().decode(&#x27;utf-8&#x27;) 读取响应对象的内容</span></span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220512204413331.png" alt="image-20220512204413331"></p><h5 id="urllib-parse"><a href="#urllib-parse" class="headerlink" title="urllib.parse"></a>urllib.parse</h5><p>百度搜索 海贼王</p><p><a href="https://www.baidu.com/s?wd=%E6%B5%B7%E8%B4%BC%E7%8E%8B">https://www.baidu.com/s?wd=%E6%B5%B7%E8%B4%BC%E7%8E%8B</a></p><p>在浏览器请求一个url的时候，浏览器会对这个url进行一个编码，除英文字母、数字和部分符号外，其他全部使用 百分号 + 十六进制 的表现形式来进行编码</p><p>注：urllib 网络模块在向一个携带有&#x3D;&#x3D;中文&#x3D;&#x3D;字样的url发起请求的时候就会出现问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url2 = <span class="string">&#x27;https://www.baidu.com/s?wd=海贼王&#x27;</span></span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(url2)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220512213833933.png" alt="image-20220512213833933"></p><p>解决方案：</p><p>把中文处理成 百分号 + 十六进制 的URL编码形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?wd=海贼王&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 字典形式</span></span><br><span class="line">r = &#123;<span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;海贼王&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">result = urllib.parse.urlencode(r)</span><br><span class="line">final_url = <span class="string">&#x27;https://www.baidu.com/s?&#x27;</span> + result</span><br><span class="line"><span class="built_in">print</span>(final_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 字符串形式</span></span><br><span class="line">r = <span class="string">&#x27;海贼王&#x27;</span></span><br><span class="line">result = urllib.parse.quote(r)</span><br><span class="line">final_url = <span class="string">&#x27;https://www.baidu.com/s?wd=&#x27;</span> + result</span><br><span class="line"><span class="built_in">print</span>(final_url)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220513190104496.png" alt="image-20220513190104496"></p><p>破解 百分号 + 十六进制 编码的地址</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;https%3A%2F%2Fshp%2Eqpic%2Ecn%2Fishow%2F2735051314%2F1652424714%5F1265602313%5F7803%5FsProdImgNo%5F2%2Ejpg%2F200&#x27;</span></span><br><span class="line">final_url = urllib.parse.unquote(url)</span><br><span class="line"><span class="built_in">print</span>(final_url)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><a href="https://shp.qpic.cn/ishow/2735051314/1652424714_1265602313_7803_sProdImgNo_2.jpg/200">https://shp.qpic.cn/ishow/2735051314/1652424714_1265602313_7803_sProdImgNo_2.jpg/200</a></p><h4 id="urllib案例练习"><a href="#urllib案例练习" class="headerlink" title="urllib案例练习"></a>urllib案例练习</h4><h5 id="爬取贴吧数据"><a href="#爬取贴吧数据" class="headerlink" title="爬取贴吧数据"></a>爬取贴吧数据</h5><p>需求：</p><ol><li>输入需要爬取的贴吧的名称（例如：海贼王）</li><li>设置爬取起始页和终止页</li><li>把每一页保存到本地</li></ol><p><strong>第一步：页面分析</strong></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220513213134320.png" alt="image-20220513213134320"></p><p>分析目标url的规律</p><p>第一页：<a href="https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B==&amp;ie=utf-8&amp;pn=0">https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B==&amp;ie=utf-8&amp;pn=0</a></p><p>第二页：<a href="https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B&amp;ie=utf-8&amp;pn=50">https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B&amp;ie=utf-8&amp;pn=50</a></p><p>第三页：<a href="https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B&amp;ie=utf-8&amp;pn=100">https://tieba.baidu.com/f?kw=%E6%B5%B7%E8%B4%BC%E7%8E%8B&amp;ie=utf-8&amp;pn=100</a></p><p>观察可以发现 </p><p>%E6%B5%B7%E8%B4%BC%E7%8E%8B 是指的海贼王</p><p>pn 表示页数 （ 值 &#x3D; (当前页数-1) x 50 ）</p><blockquote><p>要进入对于的贴 我们就修改贴吧名的URL编码</p><p>要进行翻页处理 我们就动态的提取替换pn的值</p></blockquote><p><strong>第二步：代码实现</strong></p><p>普通编写方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">name = <span class="built_in">input</span>(<span class="string">&#x27;请输入贴吧名称：&#x27;</span>)</span><br><span class="line">begin = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页：&#x27;</span>))</span><br><span class="line">end = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入终止页：&#x27;</span>))</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://tieba.baidu.com/f?&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kw = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>: name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">result = urllib.parse.urlencode(kw)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接目标url</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(begin,end+<span class="number">1</span>):</span><br><span class="line">    f_url = url + result + <span class="string">&#x27;&amp;pn=&#x27;</span> + <span class="built_in">str</span>((i-<span class="number">1</span>)*<span class="number">50</span>)</span><br><span class="line">    <span class="comment"># 构建请求对象</span></span><br><span class="line">    request = urllib.request.Request(f_url, headers=headers)</span><br><span class="line">    <span class="comment"># 获取响应对象</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    html= response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 写入文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./贴吧/&#x27;</span>+<span class="string">&#x27;第&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;页.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(html)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取第&quot;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;页&#x27;</span>)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220513230326689.png" alt="image-20220513230326689"></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220513230353794.png" alt="image-20220513230353794"></p><p>函数式的编写方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取页面</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">readPage</span>(<span class="params">url</span>):</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 构建请求对象</span></span><br><span class="line">    request = urllib.request.Request(url, headers=headers)</span><br><span class="line">    <span class="comment"># 获取响应对象</span></span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    html= response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入文档</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">writePage</span>(<span class="params">html, name</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./贴吧/&#x27;</span>+ name +<span class="string">&#x27;.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(html)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取&quot;</span> + name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    name = <span class="built_in">input</span>(<span class="string">&#x27;请输入贴吧名称：&#x27;</span>)</span><br><span class="line">    begin = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页：&#x27;</span>))</span><br><span class="line">    end = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入终止页：&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    url = <span class="string">&#x27;https://tieba.baidu.com/f?&#x27;</span></span><br><span class="line"></span><br><span class="line">    kw = &#123;</span><br><span class="line">        <span class="string">&#x27;kw&#x27;</span>: name</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result = urllib.parse.urlencode(kw)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(begin,end+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        f_url = url + result + <span class="string">&#x27;&amp;pn=&#x27;</span> + <span class="built_in">str</span>((i-<span class="number">1</span>)*<span class="number">50</span>)</span><br><span class="line">        html = readPage(f_url)</span><br><span class="line">        name = <span class="string">&#x27;第&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;页&#x27;</span></span><br><span class="line">        writePage(html, name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>面向对象的编写方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TiebaSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        self.url = <span class="string">&#x27;https://tieba.baidu.com/f?&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">readPage</span>(<span class="params">self,url</span>):</span><br><span class="line">        <span class="comment"># 构建请求对象</span></span><br><span class="line">        request = urllib.request.Request(url, headers=self.headers)</span><br><span class="line">        <span class="comment"># 获取响应对象</span></span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        html= response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">writePage</span>(<span class="params">self,html,name</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./贴吧/&#x27;</span>+ name +<span class="string">&#x27;.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(html)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;爬取&quot;</span> + name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">self</span>):</span><br><span class="line">        name = <span class="built_in">input</span>(<span class="string">&#x27;请输入贴吧名称：&#x27;</span>)</span><br><span class="line">        begin = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页：&#x27;</span>))</span><br><span class="line">        end = <span class="built_in">eval</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入终止页：&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        kw = &#123;</span><br><span class="line">            <span class="string">&#x27;kw&#x27;</span>: name</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result = urllib.parse.urlencode(kw)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(begin,end+<span class="number">1</span>):</span><br><span class="line">            </span><br><span class="line">            f_url = self.url + result + <span class="string">&#x27;&amp;pn=&#x27;</span> + <span class="built_in">str</span>((i-<span class="number">1</span>)*<span class="number">50</span>)</span><br><span class="line">            html = self.readPage(f_url)</span><br><span class="line">            name = <span class="string">&#x27;第&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;页&#x27;</span></span><br><span class="line">            self.writePage(html, name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = TiebaSpider()</span><br><span class="line">    spider.main()</span><br></pre></td></tr></table></figure><h5 id="有道翻译"><a href="#有道翻译" class="headerlink" title="有道翻译"></a>有道翻译</h5><p>需求：</p><p>通过python制作一个小翻译软件</p><p>第一步：分析页面</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514001427348.png" alt="image-20220514001427348"></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514001502198.png" alt="image-20220514001502198"></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514001941543.png" alt="image-20220514001941543"></p><p>经过分析可知</p><p>目标url：<a href="https://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule">https://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule</a></p><p>请求方式：post请求</p><p>要携带的数据：</p><blockquote><p>i: 你好</p><p>from: AUTO</p><p>to: AUTO</p><p>smartresult: dict</p><p>client: fanyideskweb</p><p>salt: 16524583458712</p><p>sign: c2f9a32b4bd8b3cbe54c2b35ffa76d51</p><p>lts: 1652458345871</p><p>bv: 247811f9b7fd387f154bf67d8ebd44f3</p><p>doctype: json</p><p>version: 2.1</p><p>keyfrom: fanyi.web</p><p>action: FY_BY_REALTlME</p></blockquote><p>得到的响应结果:</p><p>{“errorCode”:0,”translateResult”:[[{“tgt”:”hello”,”src”:”你好”}]],”type”:”zh-CHS2en”}</p><p>第二步：代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">YoudaoTranslate</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url = <span class="string">&#x27;https://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&#x27;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">readPage</span>(<span class="params">self, key</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;i&#x27;</span>: key,</span><br><span class="line">            <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;AUTO&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;AUTO&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;smartresult&#x27;</span>: <span class="string">&#x27;dict&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;client&#x27;</span>: <span class="string">&#x27;fanyideskweb&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;salt&#x27;</span>: <span class="string">&#x27;16524583458712&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;sign&#x27;</span>: <span class="string">&#x27;c2f9a32b4bd8b3cbe54c2b35ffa76d51&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;lts&#x27;</span>: <span class="string">&#x27;1652458345871&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;bv&#x27;</span>: <span class="string">&#x27;247811f9b7fd387f154bf67d8ebd44f3&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;doctype&#x27;</span>: <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;version&#x27;</span>: <span class="string">&#x27;2.1&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;keyfrom&#x27;</span>: <span class="string">&#x27;fanyi.web&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;FY_BY_REALTlME&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># data中含有中文需进行URL编码</span></span><br><span class="line">        data = urllib.parse.urlencode(data)</span><br><span class="line">        <span class="comment"># 将字符串准换为字节</span></span><br><span class="line">        data = <span class="built_in">bytes</span>(data, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        request = urllib.request.Request(</span><br><span class="line">            self.url, data=data, headers=self.headers)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">        html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取json字符串</span></span><br><span class="line">        r_dict = json.loads(html)</span><br><span class="line">        <span class="keyword">return</span> r_dict[<span class="string">&#x27;translateResult&#x27;</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="string">&#x27;src&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">self</span>):</span><br><span class="line">        key = <span class="built_in">input</span>(<span class="string">&#x27;请输入要翻译的内容：&#x27;</span>)</span><br><span class="line">        result = self.readPage(key)</span><br><span class="line">        <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = YoudaoTranslate()</span><br><span class="line">    spider.main()</span><br></pre></td></tr></table></figure><p>输入: 你好</p><p>输出: hello</p><blockquote><p>总结:</p><ol><li>urllib实现post请求时, 携带的数据含有中文时, 需要进行URL编码, 且要转换数据类型并设置 utf-8</li><li>目标的url地址去掉_o</li><li>json来解析数据 转换为字典</li></ol></blockquote><h4 id="requests模块简介"><a href="#requests模块简介" class="headerlink" title="requests模块简介"></a>requests模块简介</h4><p>第三方的http请求模块</p><p>安装方法：</p><p>pip install requests</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/1.png&#x27;</span></span><br><span class="line">req = requests.get(url)</span><br><span class="line"></span><br><span class="line">fn = <span class="built_in">open</span>(<span class="string">&#x27;./img/code.png&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">fn.write(req.content)</span><br><span class="line">fn.close()</span><br></pre></td></tr></table></figure><h4 id="requests模块快速入门"><a href="#requests模块快速入门" class="headerlink" title="requests模块快速入门"></a>requests模块快速入门</h4><h5 id="requests-get"><a href="#requests-get" class="headerlink" title="requests.get()"></a>requests.get()</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基准url不包含参数</span></span><br><span class="line">url = <span class="string">&#x27;https://tieba.baidu.com/f?&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以键值对的形式添加参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>:<span class="string">&#x27;海贼王&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pn&#x27;</span>: <span class="string">&#x27;0&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(html.response)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514180021700.png" alt="image-20220514180021700"></p><blockquote><p>response.content 返回二进制数据（字节流 bytes）</p><p>response.text 返回字符串</p><p>response.content 是直接从网站上抓取数据没有做任何处理 没有做任何解码</p><p>response.text 是resquests模块将response.content解码后得得到的字符串</p><p>resquests库会猜一个解码方式，可能会出现乱码</p><p>若出现乱码，则对requests.content 手动指定解码方式 </p><ol><li>response.content.decode(‘xxx’)</li><li>response.encoding &#x3D; xxx</li></ol></blockquote><h4 id="requests设置ip代理"><a href="#requests设置ip代理" class="headerlink" title="requests设置ip代理"></a>requests设置ip代理</h4><blockquote><ol><li><p>为什么要设置ip代理</p><p>不设置是可以的</p><p>爬虫的程序是不被认可的，会对服务器造成压力。</p><p>如果再单位时间内多次或快速访问网站数据的时候，有可能会被识别出是爬虫程序，就可能被封掉ip</p><p>如何避免？</p><p>爬虫的时候，有的时候需要控制爬取的频率，还可以使用代理ip</p><p>如何查看ip？</p><ul><li><p>cmd指令：ip config 查看 内网 局域网 ip</p></li><li><p>网站：<a href="http://httpbin.org/ip">http://httpbin.org/ip</a>  或 <a href="https://www.ipip.net/">https://www.ipip.net/</a> 查看上网用ip（要给这个ip设置代理）</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515005000129.png" alt="image-20220515005000129"></p></li></ul></li><li><p>如何设置</p><p>设置代理ip，既简单又复杂</p><p>简单 它的代码简单 添加proxies参数</p><p>复杂 能用的ip不多（免费的几乎不能用）付费（1.不会买 2.会买不会用）</p><p>获取代理ip的网站：</p><ul><li>快代理：<a href="https://www.kuaidaili.com/">https://www.kuaidaili.com/</a></li><li>豌豆代理：<a href="https://h.wandouip.com/">https://h.wandouip.com/</a></li></ul></li><li><p>代理ip的匿名度</p><ul><li>透明 服务器知道你使用了代理，也知道你真实的ip <del>最不靠谱</del></li><li>匿名 服务器知道你使用了代理，不知道你的真实ip</li><li>高匿 服务器不知道你使用了代理，也不知道你的真实ip</li></ul></li></ol><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515010131578.png" alt="image-20220515010131578"></p><ol start="4"><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 设置代理ip，免费的大部分没用</span></span><br><span class="line">ips = [</span><br><span class="line">    <span class="string">&#x27;http://114.103.88.168:894&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://220.161.243.40:766&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://115.209.170.23:36410&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://120.43.135.191:36410&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://122.232.167.237:894&#x27;</span></span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>url &#x3D; ‘<a href="http://httpbin.org/ip&#39;">http://httpbin.org/ip&#39;</a></p><p>for ip in ips:</p><pre><code>proxy = &#123;    &#39;http&#39; : ip&#125;try:    response = requests.get(url,proxies=proxy)     print(response.text)except:    print(&#39;异常&#39;)</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行结果：</span><br><span class="line"></span><br><span class="line">![image-20220515015255710](https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515015255710.png)</span><br></pre></td></tr></table></figure></li></ol></blockquote><h4 id="requests处理不被SSL证书信任的网站"><a href="#requests处理不被SSL证书信任的网站" class="headerlink" title="requests处理不被SSL证书信任的网站"></a>requests处理不被SSL证书信任的网站</h4><p>requests向一个网站发起请求的时候，它会检测数字证书。如果数字证书有问题，它会抛出一个异常 SSLError</p><p>如何处理？</p><p>将verify（默认true）设为false</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.chinatax.gov.cn/&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url, headers=headers,verify=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><h4 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h4><p>在爬虫中的作用？</p><ul><li><p>反反爬</p></li><li><p>模拟登录</p><p>在headers请求头中添加cookies键值对</p></li></ul><h4 id="requests案例练习"><a href="#requests案例练习" class="headerlink" title="requests案例练习"></a>requests案例练习</h4><h5 id="12306查票"><a href="#12306查票" class="headerlink" title="12306查票"></a><strong>12306查票</strong></h5><p>需求：</p><p>查询5月19日车次的软卧一等座情况</p><p>第一步：页面分析</p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515161900904.png" alt="image-20220515161900904"  /><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515164110089.png" alt="image-20220515164110089"></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515164034761.png" alt="image-20220515164034761"></p><p>我们发现12306的车次列表的数据是通过ajax动态加载的</p><p>目标url为</p><p><a href="https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2022-05-19&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=CSQ&amp;purpose_codes=ADULT">https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2022-05-19&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=CSQ&amp;purpose_codes=ADULT</a></p><p>发请求得到响应结果</p><p>解析结果，将数据放到列表里</p><p>通过下标索引值判断是否有票，来达到查票的需求</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2022-05-19&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=CSQ&amp;purpose_codes=ADULT&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>:<span class="string">&#x27;_uab_collina=165260260937636013111352; JSESSIONID=C862255BD8F16068C88267B870CF5AF1; BIGipServerpassport=887619850.50215.0000; guidesStatus=off; highContrastMode=defaltMode; cursorStatus=off; RAIL_EXPIRATION=1652870300386; RAIL_DEVICEID=Cibz4amAy-7B53vLTFiVkQb0n7JD2zQJNLyKm9Z0s3wusZ-w4qiKTmUqg5kqJF0V8XIvNts63nE7P3dvM_lhQgbL8Dmje-7lEn2ZiB8o9jE2f4ogFdRqrak6IXae9OTakqXipr8d3Hy0R1pv2h95blmO_E_GDKu0; route=9036359bb8a8a461c164a04f8f50b252; _jc_save_toDate=2022-05-15; _jc_save_wfdc_flag=dc; _jc_save_fromStation=%u5317%u4EAC%2CBJP; _jc_save_toStation=%u957F%u6C99%2CCSQ; _jc_save_fromDate=2022-05-19; BIGipServerotn=1624834314.64545.0000&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发现车次下标为3，软卧一等座下标为23</span></span><br><span class="line">c_list = response.json()[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;result&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> c_list:</span><br><span class="line">    s_list = i.split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> s_list[<span class="number">23</span>]==<span class="string">&#x27;无&#x27;</span> <span class="keyword">or</span> s_list[<span class="number">23</span>]==<span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        result = <span class="string">&#x27;无票&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="string">&#x27;有票 &#x27;</span>+ s_list[<span class="number">23</span>]</span><br><span class="line">    <span class="built_in">print</span>(s_list[<span class="number">3</span>],result)</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515174535013.png" alt="image-20220515174535013"></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>报错集</title>
      <link href="/2022/05/07/%E6%8A%A5%E9%94%99%E9%9B%86/"/>
      <url>/2022/05/07/%E6%8A%A5%E9%94%99%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h3 id="TypeError类型错误"><a href="#TypeError类型错误" class="headerlink" title="TypeError类型错误"></a>TypeError类型错误</h3><p><strong>TypeError</strong>: urlopen() got an unexpected keyword argument ‘headers’</p><blockquote><p>原因：</p><p>urlopen()不支持headers参数</p></blockquote><blockquote><p>解决方案：</p><p>使用urllib.request.Request()</p></blockquote><p><strong>TypeError</strong>: can’t concat str to bytes</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514010111144.png" alt="image-20220514010111144"></p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514010220049.png" alt="image-20220514010220049"></p><blockquote><p>原因:</p><p>data参数需要的是字节,而不是字符串</p></blockquote><blockquote><p>解决方案:</p><p>bytes()强制转换为字节</p></blockquote><p><strong>TypeError</strong>: string argument without an encoding</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514004949882.png" alt="image-20220514004949882"></p><blockquote><p>原因:</p><p>字符串类型转换为字节时 没有编码</p></blockquote><blockquote><p>解决方案:</p><p>bytes(data,’utf-8’)</p></blockquote><p><strong>TypeError</strong>: string indices must be integers</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220514013725881.png" alt="image-20220514013725881"></p><blockquote><p>原因:</p><p>字符串的下标必须是整数</p></blockquote><blockquote><p>解决方案:</p><p>将使用 json.loads() 读取json字符串</p></blockquote><h3 id="UnicodeEncodeError编码错误"><a href="#UnicodeEncodeError编码错误" class="headerlink" title="UnicodeEncodeError编码错误"></a>UnicodeEncodeError编码错误</h3><p><strong>UnicodeEncodeError</strong>: ‘ascii’ codec can’t encode characters in position 10-12: ordinal not in range(128)</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220512213833933.png"></p><blockquote><p>原因：</p><p>url地址带有中文</p></blockquote><blockquote><p>解决方案：</p><p>把中文处理成 百分号 + 十六进制 的形式</p></blockquote><h3 id="ProxyError代理错误"><a href="#ProxyError代理错误" class="headerlink" title="ProxyError代理错误"></a>ProxyError代理错误</h3><p><strong>ProxyError</strong>: HTTPConnectionPool(host&#x3D;’118.180.166.195’, port&#x3D;8060): Max retries exceeded with url: <a href="http://httpbin.org/ip">http://httpbin.org/ip</a> (Caused by ProxyError(‘Cannot connect to proxy.’, NewConnectionError(‘&lt;urllib3.connection.HTTPConnection object at 0x000001B7407D7220&gt;: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。’)))</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515011902544.png" alt="image-20220515011902544"></p><blockquote><p>原因：</p><p>代理ip无法使用</p></blockquote><blockquote><p>解决方案：</p><p>换可以使用的代理ip</p></blockquote><h3 id="SSLError数字证书错误"><a href="#SSLError数字证书错误" class="headerlink" title="SSLError数字证书错误"></a>SSLError数字证书错误</h3><p><strong>SSLError</strong>: HTTPSConnectionPool(host&#x3D;’<a href="http://www.chinatax.gov.cn&/#39;">www.chinatax.gov.cn&#39;</a>, port&#x3D;443): Max retries exceeded with url: &#x2F; (Caused by SSLError(SSLCertVerificationError(1, ‘[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)’)))</p><p><img src="https://blog-img-1311712983.cos.ap-nanjing.myqcloud.com/image-20220515130457791.png" alt="image-20220515130457791"></p><blockquote><p>原因：</p><p>网站没有SSL证书</p></blockquote><blockquote><p>解决方案：</p><p>将verify（默认true）设为false</p></blockquote><h3 id="InvalidURL无效网址"><a href="#InvalidURL无效网址" class="headerlink" title="InvalidURL无效网址"></a>InvalidURL无效网址</h3><p><strong>InvalidURL</strong>: Proxy URL had no scheme, should start with http:&#x2F;&#x2F; or https:&#x2F;&#x2F;</p><blockquote><p>原因：</p><p>python3.6以后，由于底层修改了url解析模式，导致proxy代理解析失败导致的</p></blockquote><blockquote><p>解决方案：</p><p>如果不使用代理，那么就可以改成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;http&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;https&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request.get(url,proxies=proxies)</span><br></pre></td></tr></table></figure><p>如果使用代理的话，就可以修改成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;http&quot;</span>:<span class="string">&quot;[http://127.0.0.1:1080&quot;</span>,](http://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">1080</span><span class="string">&quot;,/)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span>https<span class="string">&quot;:&quot;</span>[https://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">1080</span>](https://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">1080</span>/)<span class="string">&quot;,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>需要注意的是，一定要写成http:&#x2F;&#x2F;+ip+port这种形式，不能去掉前面的http:&#x2F;&#x2F;，否则就会产生错误。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 报错 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 报错 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python爬虫(二)：前导知识</title>
      <link href="/2022/05/07/Python%E7%88%AC%E8%99%AB(%E4%BA%8C)%E5%89%8D%E5%AF%BC%E7%9F%A5%E8%AF%86/"/>
      <url>/2022/05/07/Python%E7%88%AC%E8%99%AB(%E4%BA%8C)%E5%89%8D%E5%AF%BC%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h4 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h4><ul><li>每个应用程序都有自己的独立标识，那么这个表示我们称之为端口</li></ul><h4 id="通讯协议"><a href="#通讯协议" class="headerlink" title="通讯协议"></a>通讯协议</h4><ul><li>国际组织定义了通讯协议 TCP&#x2F;IP协议</li><li>所谓协议就是计算机共同遵守的规定或规则</li><li>HTTP协议（超文本传输协议）（一种通讯协议）（端口号：80）</li></ul><h4 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h4><ul><li>HTTPS &#x3D; HTTP + SSL （端口：443）（以安全为目标的HTTP通道）</li></ul><h4 id="HTTP的请求和响应"><a href="#HTTP的请求和响应" class="headerlink" title="HTTP的请求和响应"></a>HTTP的请求和响应</h4><p>General 全部的</p><p>Request URL 请求的地址</p><p>Request Method 请求方法，一般情况下网站上是什么方式，爬虫代码就写什么请求方式，但是具体情况具体分析</p><p>Status Code 状态码</p><p>如果是静态网页，数据都在源码里面</p><p>如果是动态网页，这个Request URL的去分析，查找response，去看响应结果</p><p>Response Header 服务器的响应</p><p>Request Headers 客户端的请求</p><blockquote><p>重要参数：</p><p>get 请求的方法 以及目标url参数 一般不加</p><p>Host 主机端口号、域名 可加可不加</p><p>Connection keep-alive 保持长连接 一般不加</p><p>Uer-Agent 用户代理 防止网站来检查ua 反反爬的第一步 加</p><p>Accept-Encoding 不要添加 数据会有问题</p><p>Cookie 记录用户相关信息（加不加视情况而定）（有时间限制）</p><p>referer  表明当前页面从哪个页面过来 视情况而定 有时间限制</p></blockquote><p>Query String Parameters 参数 （在Payload页面下）</p><h4 id="爬虫介绍"><a href="#爬虫介绍" class="headerlink" title="爬虫介绍"></a>爬虫介绍</h4><h4 id="什么是爬虫"><a href="#什么是爬虫" class="headerlink" title="什么是爬虫"></a>什么是爬虫</h4><ul><li>简单一句话 就是代替人去模拟浏览器进行网页操作</li></ul><h4 id="为什么需要爬虫"><a href="#为什么需要爬虫" class="headerlink" title="为什么需要爬虫"></a>为什么需要爬虫</h4><ul><li><p>为其它的程序提供数据源（搜索引擎、应用）</p></li><li><p>数据分析</p><blockquote><p>快乐 采风 高校 风景 …… 种类 收藏 多少会有点不同 视频 你喜欢什么？ 数据—&gt;分析</p><p>抖音广告越来越多 直播带货非常火 原因是什么？</p><ul><li>流量大</li><li>变现</li><li>热度 非常活跃</li></ul></blockquote></li><li><p>人工智能</p><ul><li><p>学习人工智能（门槛太高）</p><blockquote><p>1.Python高级开发水平</p><p>2.数学基础</p></blockquote></li><li><p>科技公司 高等大学</p></li><li><p>智能家居 无人驾驶 人脸识别 智能语言……</p></li></ul></li><li><p>……</p></li></ul><h4 id="公司获取数据的方式"><a href="#公司获取数据的方式" class="headerlink" title="公司获取数据的方式"></a>公司获取数据的方式</h4><ul><li>公司自有的</li><li>第三方数据平台<ul><li>免费的 百度指数</li><li>付费的 数据堂</li></ul></li><li>爬虫爬取的数据</li></ul><h4 id="Python做爬虫的优势"><a href="#Python做爬虫的优势" class="headerlink" title="Python做爬虫的优势"></a>Python做爬虫的优势</h4><ul><li>PHP：对多线程、异步支持不太好</li><li>Java：代码量大、打码笨重</li><li>C&#x2F;C++：代码量大，难以编写</li><li>Python：支持模块多、代码简洁、开发效率高（scrapy框架）</li></ul><h4 id="爬虫分类"><a href="#爬虫分类" class="headerlink" title="爬虫分类"></a>爬虫分类</h4><ul><li>通用网络爬虫 例如 百度 谷歌</li><li>聚焦网络爬虫  根据既定的目标 有选择的抓取某一特定主题的内容</li></ul><h4 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h4><p>get和post方法</p><ul><li><p>get特点</p><blockquote><p>一般情况下只从服务器获取数据，没有对服务器产生影响，通常都是get请求</p><p>请求参数可以在url地址上显示出来</p></blockquote></li><li><p>post特点</p><blockquote><p>向服务器发送数据（登录、上传文件……）</p><p>会对服务器产生影响那么通常都是使用的是post请求</p><p>请求参数不会再url地址之上显示出来</p></blockquote></li></ul><h4 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h4><p>全球统一资源定位符</p><p><a href="https://gufanstudy.github.io/archives/">https://gufanstudy.github.io/archives/</a></p><blockquote><p>https 网络协议</p><p>gufanstudy.github.io 域名</p><p>archives&#x2F; 访问资源路径</p><p>anchor 锚点（前端页面定位）</p></blockquote><h4 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h4><p>User-Agent: Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;101.0.4951.54 Safari&#x2F;537.36</p><p>作用：记录了用户的浏览器，操作系统登，为了让用户更好的获取HTML页面效果</p><p>一般会作为反反爬的第一步，80%的网站都会检查</p><h4 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h4><p>记录用户相关的信息</p><p>http本身是无状态的，服务器无法判断用户的身份。对同一个网站发起多次请求，服务器无法判断是否来自同一个人</p><p>作用：</p><ul><li>反反爬</li><li>模拟登录</li></ul><h4 id="状态码"><a href="#状态码" class="headerlink" title="状态码"></a>状态码</h4><p>部分：</p><ul><li>200：请求成功</li><li>301：永久重定向</li><li>302：临时重定向</li><li>404：请求失败</li><li>500：服务器内部请求</li></ul><p>重定向：</p><p>一个网址定向到另一个网址</p><h4 id="抓包工具"><a href="#抓包工具" class="headerlink" title="抓包工具"></a>抓包工具</h4><p>谷歌浏览器自带，右键-检查 或 F12</p><p>Elements 元素</p><blockquote><p>网页源代码 最终渲染结</p><p>用于提取数据 分析数据</p></blockquote><p>Console 控制台</p><blockquote><p>后期分析JS代码，根据打印结果，分析代码</p><p>前期用不到</p></blockquote><p>Sources 资源</p><blockquote><p>信息的来源</p><p>整个网站加载的资源</p><p>分析JS代码时使用，调试，打断点</p><p>前期用不到</p></blockquote><p>Network 网络工作</p><blockquote><p>数据抓包</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python爬虫(一)：学习路线</title>
      <link href="/2022/05/06/Python%E7%88%AC%E8%99%AB(%E4%B8%80)%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
      <url>/2022/05/06/Python%E7%88%AC%E8%99%AB(%E4%B8%80)%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<h2 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h2><blockquote><p>打铁还需自身硬</p><p>前置要求：掌握Python基础知识</p></blockquote><ul><li><p>知识点+案例 为驱动</p></li><li><p>第一阶段：爬虫核心</p><ul><li><p>爬虫的前导知识</p></li><li><p>爬虫网络请求模块（uellib requests）</p></li><li><p>数据解析（正则 xpath bas4）</p></li></ul></li><li><p>第二阶段：爬虫进阶</p><ul><li><p>selenium(自动化的测试工具)</p></li><li><p>动态数据</p></li><li><p>多线程</p></li><li><p>增加爬取的效率</p></li><li><p>队列</p></li></ul></li><li><p>第三阶段：Scrapy框架和分布式爬虫</p><ul><li>如何使用Scrapy框架</li><li>分布式爬虫工作流程以及如何把一个普通的爬虫改写成分布式爬虫</li></ul></li><li><p>第四阶段：数据存储</p><ul><li>CSV</li><li>redis</li><li>mongodb（重点）</li></ul></li><li><p>第五阶段：移动开发</p><ul><li>前导知识（Android）（mainactivity listview textview）</li><li>fiddler抓包工具</li><li>爬取移动端数据</li></ul></li><li><p>第六阶段：反爬策略</p><ul><li>字体反爬</li><li>代理IP（免费代理IP 失效率低 响应慢）（购买IP）</li><li>复杂验证码（图片验证 tessertact 行为验证 打码平台）</li><li>JS反爬（门槛相当高 起码掌握JS基础 分析维度比较高 案例 总结套路 常见算法了解）</li></ul></li></ul><blockquote><p>PS：</p><p>​懂了</p><p>​不代表会了</p></blockquote><blockquote><p>学习方法：</p><p>​百度  未知的 不明确的 找资料</p><ul><li>准备报错集</li><li>解决方案文档</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>传统人脸识别方法介绍</title>
      <link href="/2022/05/03/%E4%BC%A0%E7%BB%9F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/"/>
      <url>/2022/05/03/%E4%BC%A0%E7%BB%9F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h3 id="基于几何特征的方法"><a href="#基于几何特征的方法" class="headerlink" title="基于几何特征的方法"></a>基于几何特征的方法</h3><p>基于几何特征的方法是最早、最传统的方法，该方法是通过提取人脸的几何特征，包括人脸部件的归一化的点间距离、比率以及人脸的一些特征点，如眼角、嘴角、鼻尖等部位所构成的二维拓扑结构进行识别的方法。主要代表是MIT的Bruelli和Poggio小组,他们采用改进的积分投影法提取出用欧氏距离表征的35维人脸特征矢量用于模式分类。几何特征方法利用的是单纯的几何信息，其优点是所需的存储空间和分类时间代价比较小，而且在图像分辨率较低的情况下仍然可以使用。缺点是识别的准确率完全依赖于几何特征的准确提取，而几何特征本身容易受姿态和表情变化的影响，稳定性不高。即使几何特征提取准确，仍然可能因为有较大的变化而导致分类错误。因此，目前单纯利用几何特征的人脸识别方法已经不常使用了，但几何特征仍然可以作为其它特征的有益补充。</p><blockquote><p>优点：</p><p>所需存储空间和分类时间代价比较小</p><p>在图像分辨率低的情况下也可以使用</p></blockquote><blockquote><p>缺点：</p><p>不稳定</p><p>精度完全依赖几何特征的准确提取，容易受人脸的姿态和表情影响目前很少单独使用</p><p>往往作为其他特征的补充</p></blockquote><h3 id="基于模板匹配的方法"><a href="#基于模板匹配的方法" class="headerlink" title="基于模板匹配的方法"></a>基于模板匹配的方法</h3><p>模板匹配的方法是模式识别中所采用方法中最传统的方法之一，该方法主要是利用计算模板和图像灰度的自相关性来实现识别功能的。主要代表是Yuille,他采用弹性模板来提取眼睛和嘴巴的轮廓。在人脸识别问题中，模板既可以是整个人脸图像构成的单-模板，也可以是不同局部特征子图像构成的多个模板的组合。分类时，将待识别图像与所有已知图像进行匹配，根据最终匹配得分进行分类。Brunelli 在1993 年对基于几何特征的方法和模板匹配的方法进行了较全面的介绍和比较后认为,模板匹配的方法优于几何特征的方法.但是，模板匹配的方法由于利用了相关性信息，对光照、旋转和表情变化比较敏感。另外，模板匹配的计算量比较大，多尺度、多模板的使用会增加计算和存储的复杂度。</p><blockquote><p>优点：</p><p>优于几何特征信息</p></blockquote><blockquote><p>缺点：</p><p>由于利用了相关信息，对光照、旋转和表情变化比较敏感</p><p>计算量大，在多尺度、多模板的情况下会增加计算和存储的复杂性</p></blockquote><h3 id="基于统计的方法"><a href="#基于统计的方法" class="headerlink" title="基于统计的方法"></a>基于统计的方法</h3><p>基于统计的方法是现代人脸识别方法中比较经典且常见的，包括特征脸Eganc方法和隐马尔可夫模型(iden Mantov Mole等方法。基于统计的方法将人险用代数特征矢量表示。这类方法利用统计策略从整个训练图像集合提取统计特征后，通过匹配学习找出人脸和非人脸以及不同人脸之间的联系，并不要求单独抽取眼睛、鼻子等面部特征，从原理上更为先进合理，实验中也表现出更好的识别效果。</p><h4 id="——K-L变换的特征脸"><a href="#——K-L变换的特征脸" class="headerlink" title="——K-L变换的特征脸"></a>——K-L变换的特征脸</h4><p>特征脸方法,也称为主分量分析(PCA)方法。srvich和Kiby)首先将K-L变换用于人脸图像的最优表示，Turk 受其启发，将包含人脸的图像区域看作是一种随机向量， 通过KL变换获得其正交基底，并利用重构权向量作为识别用的特征，从而提出了“特征脸”方法。该方法具有简单有效的特点，目前已成为事实上的基准测试算法。本文的第三章将重点讨论此方法的具体应用。特征脸方法的重要贡献不仅在于它为基于图像的目标识别提供了一一种普遍的方法， 而且降维和特征提取这种思想为目标识别提供了- -种方法论。但是在人脸识别问题上，它的不足之处是受表情变化、光照角度、强度变化和视角变化等严重影响，鲁棒性较差。因此,研究者在此基础上发展了许多改进方案，如Behumer提出的Fisher脸方法,特征半脸方法四等。</p><blockquote><p>优点：</p><p>简单有效</p></blockquote><blockquote><p>缺点：</p><p>鲁棒性差，易受表情、光照、角度等因素影响</p></blockquote><h4 id="——隐马尔可夫模型"><a href="#——隐马尔可夫模型" class="headerlink" title="——隐马尔可夫模型"></a>——隐马尔可夫模型</h4><p>隐马尔可夫模型是用于描述信号统计特征的一组统计模型， 作为一种语音识别的方法取得了很好的效果。近年来越来越多的研究工作者将这种方法运用到人脸识别当中。Samaria,叫最早建议了关于人脸的一维隐马尔可夫模型，他用一个矩形窗从上到下采样人脸图像,用灰度值作为观察序列.Netan发展了Samria的方法，用39个2D-0T系数代替灰度值作为观察序列，在一定程度上解决了Samaria方法要求大存储空间的不足。并且有效地提高了识率。</p><blockquote><p>优点：</p><p>在一定程度上解决了Samaria方法要求大存储空间的不足。并且有效地提高了识率。</p></blockquote><h3 id="基于链接机制的方法"><a href="#基于链接机制的方法" class="headerlink" title="基于链接机制的方法"></a>基于链接机制的方法</h3><p>基于连接机制的方法，包括一般的神经网络(Neural Network)方法和弹性图匹配(Elastic Graph Matching)方法21。基于神经网络的方法是最近几年比较活跃的一个研究方向。神经网络由于其固有的并行运算机制以及对于模式的分布式全局存储，在人脸识别上比其它类别的方法有独到的优势，它避免了复杂的特征提取作，通过学习的过程获得其它方法难以实现的关于人脸识别规律和规则的隐性表达，而且不受模式形变的影响。最早应用神经网络进行人脸识别工作的Kohonen,利用网络的联想能力回忆人脸，当输入图像噪音很多或部分图像丢失时，也能回忆出准确的人脸。但是该方法通常将人脸作为一个- -维向量输入，如一幅不大100x100的图像为10000维，这样神经网络的输入节点将很庞大，因此实际训练网络的时候参数繁多，实现起来很困难。同时该方法要求训练样本比较充分才能得到较好的识别效果，否则在光照、尺度变化以及小的形变情况下都不能很好地识别。</p><h4 id="——动态链接的弹性图匹配"><a href="#——动态链接的弹性图匹配" class="headerlink" title="——动态链接的弹性图匹配"></a>——动态链接的弹性图匹配</h4><p>弹性图匹配方法是一-种基于动态链接结构的方法。该方法在二维空间中定义了一种对 于通常的人脸变形具有一定的不变性的距离，并采用属性拓扑图来代表人脸，拓扑图的任一项点均包含特征向量，用来记录人脸在该项点位置附近的特征信息，然后利用弹性匹配法将库中人脸和待识别人脸的弹性图进行匹配，找到匹配程度最高的一个人脸图像。该方法结合了灰度特性和几何因素，在匹配时允许图像存在弹性形变，对人脸较小角度的旋转、表情以及光照变化等都有较好的容忍性，但是与特征脸识别方法相比识别速度较慢。</p><blockquote><p> 优点：</p><p>结合了灰度特性和几何因素，允许图像存在形变，对小角度旋转、表情和光照变化有较好的容忍性</p></blockquote><blockquote><p>缺点：识别速度较慢</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 人脸识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 素材 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
